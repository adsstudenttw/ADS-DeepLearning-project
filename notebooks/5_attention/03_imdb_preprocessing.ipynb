{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mltrainer import tokenizer\n",
    "import mltrainer\n",
    "mltrainer.__version__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset. This is the MNIST for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "imdbdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.IMDB)\n",
    "datasets = imdbdatasetfactory.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = datasets[\"train\"]\n",
    "testdataset = datasets[\"valid\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 50k movie reviews, labeled positive or negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look at the first datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = traindataset[0]\n",
    "x, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy data. We have Uppercase, punctuation, and even html tags. Let's clean that out in order to reduce dimensionality, without loosing too much information about the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation = f\"[{string.punctuation}]\"\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    punctuation = f\"[{string.punctuation}]\"\n",
    "    # remove CaPiTaLs\n",
    "    lowercase = text.lower()\n",
    "    # change don't and isn't into dont and isnt\n",
    "    neg = re.sub(\"\\\\'\", \"\", lowercase)\n",
    "    # swap html tags for spaces\n",
    "    html = re.sub(\"<br />\", \" \", neg)\n",
    "    # swap punctuation for spaces\n",
    "    stripped = re.sub(punctuation, \" \", html)\n",
    "    # remove extra spaces\n",
    "    spaces = re.sub(\"  +\", \" \", stripped)\n",
    "    return spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(x), y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now we need to create a vocabulary, which is a mapping from every unique word to an arbitrary integer. We have seen this in lesson 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import tokenizer\n",
    "\n",
    "v = tokenizer.build_vocab(corpus, max=10000)\n",
    "len(v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after cleaning, we have about 80k unique tokens. This is even more without the cleaning, because \"The\" and \"the\" will be two different tokens.\n",
    "\n",
    "We also have tokens for unknown words, and for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[\"<UNK>\"], v[\"<PAD>\"], v[\"sdflkjl\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps a sentence of words to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v[word] for word in clean(x).split()[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Callable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self, max: int, vocab: Vocab, clean: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.max = max\n",
    "        self.vocab = vocab\n",
    "        self.clean = clean\n",
    "\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"neg\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def __call__(self, batch: List) -> Tuple[Tensor, Tensor]:\n",
    "        labels, text = [], []\n",
    "        for x, y in batch:\n",
    "            if clean is not None:\n",
    "                x = self.clean(x)\n",
    "            x = x.split()[: self.max]\n",
    "            tokens = torch.tensor([self.vocab[word] for word in x])\n",
    "            text.append(tokens)\n",
    "            labels.append(self.cast_label(y))\n",
    "\n",
    "        text_ = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "        return text_, torch.tensor(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is necessary to:\n",
    "- cut of long sentences to get equal length. 100 words will be enough to get the sentiment in most cases\n",
    "- we need to cast the labels \"neg\" and \"pos\" to integers\n",
    "- we also pad if a sentence is shorter than the max lenght\n",
    "\n",
    "We can feed the preprocessor to the default dataloader from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "preprocessor = Preprocessor(max=100, vocab=v, clean=clean)\n",
    "dataloader = DataLoader(\n",
    "    traindataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get batched sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this code is wrapped into the DatasetFactoryProvider, which you can see in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
