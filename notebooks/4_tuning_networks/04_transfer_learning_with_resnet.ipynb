{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the flowers dataset from the first lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "flowersfactory = DatasetFactoryProvider.create_factory(DatasetType.FLOWERS)\n",
    "streamers = flowersfactory.create_datastreamer(batchsize=32, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FlowersDatasetFactory adds a transform function that will (at random) flip, scale, affine an image. For the model, this is a new image, so it needs to focus on the general patterns instead of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train.batchloop()\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(trainstreamer)\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = X.permute(0, 2, 3, 1)\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10,10))\n",
    "axs = axs.ravel()\n",
    "for i in range(9):\n",
    "    axs[i].imshow(img[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the ranges, mean and std of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.max(), X.min(), X.mean(), X.std()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building our own resnet, we will just download a pretrained version. This saves us many hours of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "resnet = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = resnet(X)\n",
    "yhat.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the resnet is trained for 1000 classes. We have just 5...\n",
    "\n",
    "We will swap the last layer and retrain the model.\n",
    "\n",
    "First, we freeze all pretrained layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you study the resnet implementation on [github](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L206) you can see that the last layer is named `.fc`, like this:\n",
    "\n",
    "```\n",
    " self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    " ```\n",
    "\n",
    "This is a Linear layer, mapping from 512 * block.expansion to num_classes.\n",
    "\n",
    "\n",
    "so we will swap that for our own. To do so we need to figure out how many features go into the .fc layer.\n",
    "We can retrieve the incoming amount of features for the current `.fc` with `.in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resnet.fc))\n",
    "in_features = resnet.fc.in_features\n",
    "in_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's swap that layer with a small, two layer, neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, 5)\n",
    "    # nn.Linear(in_features, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128, 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = resnet(X)\n",
    "yhat.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a fully trained resnet, but we added two layers at the end that transforms everything into 5 classes.\n",
    "These layers are random, so we need to train them for some epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to train (about 4 min per epoch), you could scale down to amount of trainsteps to speed things up.\n",
    "\n",
    "You will start with a fairly high learning rate (0.01), and if the learning stops, after patience epochs the learning rate gets halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellog\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=resnet,\n",
    "    settings=settings,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this will be very slow!\n",
    "trainer.loop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
