{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises \n",
    "\n",
    "## 0. Setup your own repo\n",
    "- use the `cookiecutter` command that is preinstalled on your VM to create a repo, see https://github.com/raoulg/datascience-cookiecutter for details,\n",
    "- push your own repo to github. Use `ML23-{yourname}` as a format, eg `ML23-joostb`.\n",
    "- Invite me (raoulg) as a collaborator to your repo.\n",
    "- make the excercises 1-5 below in your repo, and push them to github.\n",
    "\n",
    "Tips:\n",
    "- Run `make format` and `make lint` before you commit to improve your code\n",
    "- Commit often (every 30 minutes or so)\n",
    "- Commit groups of files that are related to each other. If you have more files, commit them separately.\n",
    "- Write commit messages that are descriptive and informative. \"lesson 1\" is a bad commit message. \"added excercise 2\" is better, \"[ex2] added __len__ to Dataset class\" is even better.\n",
    "- Use `pdm` to add dependencies. `mads_datasets` and `mltrainer` should be enough, because mltrainer also\n",
    "\n",
    "At some point, you will get a grade for the excercises that is 0 (not good enough), 1 (good enough) or 2 (excellent).\n",
    "I will look for both form and correctness to determine your grade.\n",
    "The result be incorporated into your final grade for this course.\n",
    "\n",
    "## 1. Computational graph\n",
    "\n",
    "Read p29 of your book. Can you write down the formula of the computational graph?\n",
    "In other words: what mathematical function is this computational graph computing?\n",
    "\n",
    "## 2. 3D Tensor dataset\n",
    "Create a random 3D tensor dataset with `torch` and build your own `DataSet` class.\n",
    "See p35-36 of the book and notebook 03_dataloader.\n",
    "\n",
    "## 3. Datastreamers\n",
    "Study the `BaseDatastreamer` in `03_dataloader` and use it with your own dataset.\n",
    "\n",
    "## 4. Cross entropy\n",
    "Recreate the function at p50 (cross_entropy) and visualize it for multiple combinations of label and prediction.\n",
    "\n",
    "# 5. Tune the network\n",
    "Run the experiment below, and study the result with tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "import gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `gin-config` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "\n",
    "The `model.gin` file is a simple file that will try to load parameters for funcitons that are already imported. \n",
    "\n",
    "So, if you wouldnt have imported train_model, the ginfile would not be able to parse settings for train_model.trainloop and will give an error.\n",
    "\n",
    "We can print all the settings that are operational with `gin.operative_config_str()` once we have loaded the functions to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = BasePreprocessor()\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gin.config_str())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big advantage is that we can save this config as a file; that way it is easy to track what you changed during your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "units = [256, 128, 64]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=5,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN],\n",
    ")\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        gin.bind_parameter(\"NeuralNetwork.units1\", unit1)\n",
    "        gin.bind_parameter(\"NeuralNetwork.units2\", unit2)\n",
    "\n",
    "        model = imagemodels.NeuralNetwork()\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, `cd` to the location of the repository\n",
    "- activate the python environment for the shell. Note how the correct environment is being activated.\n",
    "- run `tensorboard --logdir=models` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Experiment with things like:\n",
    "\n",
    "- changing the amount of units1 and units2 to values between 16 and 1024. Use factors of 2: 16, 32, 64, etc.\n",
    "- changing the batchsize to values between 4 and 128. Again, use factors of two.\n",
    "- all your experiments are saved in the `models` directory, with a timestamp. Inside you find a saved_config.gin file, that \n",
    "contains all the settings for that experiment. The `events` file is what tensorboard will show.\n",
    "- plot the result in a heatmap: units vs batchsize.\n",
    "- changing the learningrate to values between 1e-2 and 1e-5 \n",
    "- changing the optimizer from SGD to one of the other available algoritms at [torch](https://pytorch.org/docs/stable/optim.html) (scroll down for the algorithms)\n",
    "\n",
    "A note on train_steps: this is a setting that determines how often you get an update. \n",
    "Because our complete dataset is 938 (60000 / 64) batches long, you will need 938 trainstep to cover the complete 60.000 images.\n",
    "\n",
    "This can actually be a bit confusion, because every value below 938 changes the meaning of `epoch` slightly, because one epoch is no longer\n",
    "the full dataset, but simply `trainstep` batches. Setting trainsteps to 100 means you need to wait twice as long before you get feedback on the performance,\n",
    "as compared to trainsteps=50. You will also see that settings trainsteps to 100 improves the learning, but that is simply because the model has seen twice as \n",
    "much examples as compared to trainsteps=50.\n",
    "\n",
    "This implies that it is not usefull to compare trainsteps=50 and trainsteps=100, because setting it to 100 will always be better.\n",
    "Just pick an amount, and adjust your number of epochs accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
