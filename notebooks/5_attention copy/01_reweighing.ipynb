{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweighing based on proximity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some simple timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 4 * torch.pi, 100)\n",
    "\n",
    "\n",
    "def f(t):\n",
    "    noise = torch.randn(100) * 0.1\n",
    "    return torch.sin(t) + noise\n",
    "\n",
    "\n",
    "x = f(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create a filter that smooths the signal by taking in the neighbouring signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a convolution\n",
    "conv = torch.nn.Conv1d(1, 1, 8)\n",
    "\n",
    "# build a gaussian kernel\n",
    "n = torch.distributions.normal.Normal(0, 1)\n",
    "v = torch.arange(-4, 4)\n",
    "gaussian = torch.exp(n.log_prob(v))[None, None, :]\n",
    "\n",
    "# replace the random weights\n",
    "d = conv.state_dict()\n",
    "d[\"weight\"] = gaussian\n",
    "conv.load_state_dict(d)\n",
    "x_ = conv(x[None, None, :])[0][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot everything like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(t, x)\n",
    "ax[0].set_title(\"original\")\n",
    "\n",
    "ax[1].scatter([*range(8)], gaussian[0][0])\n",
    "ax[1].set_title(\"gaussian filter\")\n",
    "\n",
    "ax[2].plot(x_.detach())\n",
    "ax[2].set_title(\"smoothed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you see a noisy signal. The gaussian filter takes adds more of the close values, and less of the values further away. The filter works on the direct neighborhood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import tokenizer\n",
    "\n",
    "corpus = [\"ik zit op de bank\", \"ik werk bij de bank\", \"bij de bank is het heel druk\"]\n",
    "v = tokenizer.build_vocab(corpus, max=11)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the problem with text is: it is not necessarily the words that are close, that have the most impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[\"bank\"], v[\"test\"], len(v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see three sentences, max seven words, so dimensions are (3,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.tokenize(corpus, v)\n",
    "x, x.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sentences are being encoded, and the word \"bank\" gets the integer 6 assigned. However, the meaning of this word is not the same because of the context... If we make an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(num_embeddings=len(v), embedding_dim=4, padding_idx=0)\n",
    "\n",
    "embeddings = emb(x)\n",
    "\n",
    "embeddings, embeddings.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a dimensionality of 4 to every word. So now we have (3, 7, 4).\n",
    "You can see that the word \"bank\" gets exactly the same vector, as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1 = embeddings[0][4]\n",
    "bank2 = embeddings[1][4]\n",
    "bank1, bank2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Now we will start with the attention mechanism.\n",
    "We need a key, query and value. Because we use self attention, these are just clones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = embeddings.detach().clone()\n",
    "query = embeddings.detach().clone()\n",
    "values = embeddings.detach().clone()\n",
    "key.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = torch.tensor(query.shape[-1])\n",
    "d_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this, we can calculate $$\\frac{(QK^T)}{\\sqrt{d}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.bmm(query, key.transpose(1, 2)) / torch.sqrt(d_features)\n",
    "dots.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a shape of (3, 7, 7):\n",
    "for every sentence, we have for every word, weights how we want to mix in every other word. So this last part always has a shape (sequence, sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the weights with a softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = nn.Softmax(dim=-1)(dots)\n",
    "\n",
    "weights[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally we can do a matrix-multiplication with the values:\n",
    "\n",
    "$$attention = softmax\\left(\\frac{(QK^T)}{\\sqrt{d}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.bmm(weights, values)\n",
    "activations.shape, embeddings.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note how we end up with exactly the same size: 3 sentences, max 7 words, but now every word has 4 dimensions that are reweighted by the other words in the sentence, regardless of the distance, but mainly depending on the semantics (meaning) of every word as encoded in de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1 = activations[0][4]\n",
    "bank2 = activations[1][4]\n",
    "bank1, bank2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the vector for the word bank has been \"mixed\" with all the other words in the sentence, and they are different!\n",
    "\n",
    "torch has a multihead attention implemented. With that, we can add a mask to cover the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = x == 0\n",
    "mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = nn.MultiheadAttention(embed_dim=4, num_heads=2, batch_first=True)\n",
    "attn, attn_w = multihead(query, key, values, key_padding_mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to visualize the weights. In this case, this is untrained.\n",
    "What you expect is that e.g. the vector for the word \"bank\" should be mixed with the word \"zit\" (sit) to make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = corpus[0].split()\n",
    "labels = labels + [\"PAD\", \"PAD\"]\n",
    "\n",
    "plot = sns.heatmap(attn_w[0].detach().numpy())\n",
    "\n",
    "plot.set_xticklabels(labels);\n",
    "plot.set_yticklabels(labels);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "A final ingredient is positional encoding.\n",
    "\n",
    "With Attention we loose all sense of location in a sentence. A way around this is to add positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return x\n",
    "\n",
    "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(pe, cmap=\"RdGy\", extent=(1, pe.shape[1] + 1, pe.shape[0] + 1, 1))\n",
    "# fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1] + [i * 10 for i in range(1, 1 + pe.shape[1] // 10)])\n",
    "ax.set_yticks([1] + [i * 10 for i in range(1, 1 + pe.shape[0] // 10)])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to add multiple sine waves.\n",
    "One sinewave will have a very fast frequency, another sinewave will have a slow frequency.\n",
    "Note that this is exactly how our time-system works: minutes have a fast frequency, hours a slower one, days of the week even slower etc.\n",
    "\n",
    "By combining a lot of sinewaves the model is able to figure out something about the location in time, relative to each other. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
