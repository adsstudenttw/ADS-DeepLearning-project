{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Tuple\n",
    "from mads_datasets import datatools\n",
    "from pathlib import Path\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Protocol, List, Any, Mapping, Iterator, Optional, Callable, Sequence\n",
    "from abc import ABC, abstractmethod\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProtocol(Protocol):\n",
    "    def __len__(self) -> int:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        ...\n",
    "\n",
    "class ProcessingDatasetProtocol(DatasetProtocol):\n",
    "    def process_data(self) -> None:\n",
    "        ...\n",
    "\n",
    "class DataStreamerProtocol(Protocol):\n",
    "    def stream(self) -> Iterator:\n",
    "        ...\n",
    "\n",
    "class PreprocessorProtocol(Protocol):\n",
    "    def __call__(self, batch: List[tuple]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        ...\n",
    "\n",
    "class AbstractDataset(ProcessingDatasetProtocol):\n",
    "    def __init__(self, data: Tuple) -> None:\n",
    "        self.dataset: List = []\n",
    "        self.process_data(data)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        return self.dataset[index]\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process_data(self, data: List) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EegDataset(AbstractDataset):\n",
    "    def process_data(self, data: ndarray) -> None:\n",
    "\n",
    "        for set in data:\n",
    "            set = set.tolist()\n",
    "            self.dataset.append((torch.tensor(set[:-1]), torch.tensor(int(set[-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreprocessor(PreprocessorProtocol):\n",
    "    def __call__(self, batch: list[tuple]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        X, y = zip(*batch)\n",
    "        return torch.stack(X), torch.stack(y)\n",
    "\n",
    "\n",
    "class PaddedPreprocessor(PreprocessorProtocol):\n",
    "    def __call__(self, batch: list[tuple]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        X, y = zip(*batch)\n",
    "        X_ = pad_sequence(X, batch_first=True, padding_value=0)  # noqa N806\n",
    "        return X_, torch.tensor(y)\n",
    "\n",
    "class WindowingPreprocessor(PreprocessorProtocol):\n",
    "    def __init__(self, window_size: int):\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __call__(self, batch: list[tuple]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        X, y = zip(*batch)\n",
    "\n",
    "        X_windowed = [self.window_sequence(seq) for seq in X]\n",
    "        X_padded = pad_sequence(X_windowed, batch_first=True, padding_value=0)\n",
    "\n",
    "        return X_padded, torch.stack(y)\n",
    "\n",
    "    def window_sequence(self, sequence):\n",
    "        windows = [sequence[i:i + self.window_size] for i in range(0, len(sequence), self.window_size)]\n",
    "        return torch.stack(windows)\n",
    "\n",
    "class BaseDatastreamer(DataStreamerProtocol):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: DatasetProtocol, \n",
    "        batch_size: int, \n",
    "        preprocessor: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if preprocessor == None:\n",
    "            self.preprocessor = lambda x: zip(*x)\n",
    "        else:\n",
    "            self.preprocessor = preprocessor\n",
    "        \n",
    "        self.size = len(self.dataset)\n",
    "        self.reset_index()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.dataset) / self.batch_size)\n",
    "    \n",
    "    def reset_index(self) -> None:\n",
    "        self.index_list = np.random.permutation(self.size)\n",
    "        self.index = 0\n",
    "    \n",
    "    def batchloop(self) -> Sequence[Tuple]:\n",
    "        batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            x, y = self.dataset[self.index_list[self.index]]\n",
    "            batch.append((x, y))\n",
    "            self.index += 1\n",
    "        return batch\n",
    "    \n",
    "    def stream(self) -> Iterator:\n",
    "        if self.index > (self.size - self.batch_size):\n",
    "            self.reset_index()\n",
    "        batch = self.batchloop()\n",
    "        X, Y = self.preprocessor(batch)\n",
    "        yield X, Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSettings(BaseModel):\n",
    "    dataset_url: str\n",
    "    data_dir: Path\n",
    "    filename: Path\n",
    "    name: str\n",
    "    unzip: bool\n",
    "    \n",
    "eegDatasetSettings = DatasetSettings(\n",
    "    dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\",\n",
    "    data_dir = Path(\"../data/eeg\").resolve(),\n",
    "    filename = Path(\"eeg.arff\"),\n",
    "    name = \"EEG\",\n",
    "    unzip = False\n",
    ")\n",
    "\n",
    "class AbstractDatasetFactory(ABC):\n",
    "    def __init__(self, settings: DatasetSettings) -> None:\n",
    "        self.settings = settings\n",
    "    \n",
    "    def download_data(self) -> None:\n",
    "        data_dir = self.settings.data_dir\n",
    "        filename = self.settings.filename\n",
    "        data_path = data_dir / filename\n",
    "\n",
    "        if not data_path.exists():\n",
    "            data_dir.mkdir(parents=True)\n",
    "            datatools.get_file(\n",
    "                data_dir=data_dir, \n",
    "                filename=filename, \n",
    "                url=self.settings.dataset_url, \n",
    "                unzip=False\n",
    "            )\n",
    "    \n",
    "    @abstractmethod\n",
    "    def create_dataset(self) -> Mapping[str, DatasetProtocol]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def create_datastreamer(\n",
    "            self, batch_size: int, **kwargs\n",
    "        ) -> Mapping[str, DataStreamerProtocol]:\n",
    "        datasets = self.create_dataset()\n",
    "        train_dataset = datasets[\"train\"]\n",
    "        valid_dataset = datasets[\"valid\"]\n",
    "        preprocessor: Optional[Callable] = kwargs.pop(\"preprocessor\", None)\n",
    "\n",
    "        train_streamer = BaseDatastreamer(\n",
    "            train_dataset, batch_size=batch_size, preprocessor=preprocessor\n",
    "        )\n",
    "        valid_streamer = BaseDatastreamer(\n",
    "            valid_dataset, batch_size=batch_size, preprocessor=preprocessor\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"train\": train_streamer,\n",
    "            \"valid\": valid_streamer\n",
    "        }\n",
    "\n",
    "class EegDatasetFactory(AbstractDatasetFactory):\n",
    "    def __init__(self, settings: DatasetSettings = eegDatasetSettings) -> None:\n",
    "        super().__init__(settings)\n",
    "        self.datasets = Mapping[str, DatasetProtocol]\n",
    "    \n",
    "    def create_dataset(self) -> Mapping[str, DatasetProtocol]:\n",
    "        self.download_data()\n",
    "\n",
    "        data_path = self.settings.data_dir / self.settings.filename\n",
    "        dataset = arff.loadarff(data_path)[0]\n",
    "\n",
    "        split = int(0.8 * len(dataset))\n",
    "        train_dataset = EegDataset(dataset[:split])\n",
    "        valid_dataset = EegDataset(dataset[split:])\n",
    "\n",
    "        datasets = {\n",
    "            \"train\": train_dataset,\n",
    "            \"valid\": valid_dataset\n",
    "        }\n",
    "        self.datasets = datasets\n",
    "\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegDatasetFactory = EegDatasetFactory()\n",
    "eegDataset = eegDatasetFactory.create_dataset()\n",
    "eegdatastreamer = eegDatasetFactory.create_datastreamer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([4289.2300, 4011.7900, 4248.2100, 4134.3599, 4338.9702, 4622.5601,\n",
       "          4079.4900, 4603.5898, 4192.8198, 4233.3301, 4195.3799, 4274.8701,\n",
       "          4587.6899, 4358.4600]),\n",
       "  tensor([4307.6899, 4007.1799, 4245.6401, 4120.5098, 4340.0000, 4632.3101,\n",
       "          4084.6201, 4603.0801, 4188.7202, 4237.4399, 4197.9502, 4282.5601,\n",
       "          4607.1802, 4380.5098]),\n",
       "  tensor([4306.6699, 4002.5601, 4259.4902, 4116.4102, 4328.7202, 4605.6401,\n",
       "          4056.4099, 4614.3599, 4187.1802, 4223.5898, 4187.1802, 4265.6401,\n",
       "          4607.6899, 4362.0498]),\n",
       "  tensor([4269.2300, 3985.6399, 4245.1299, 4109.2300, 4320.0000, 4625.6401,\n",
       "          4058.9700, 4610.7700, 4196.4102, 4230.2598, 4195.8999, 4265.1299,\n",
       "          4606.1499, 4339.4902]),\n",
       "  tensor([4275.3799, 3968.2100, 4249.2300, 4092.8201, 4321.5400, 4598.9702,\n",
       "          4036.9199, 4590.7700, 4176.4102, 4221.5400, 4184.1001, 4249.7402,\n",
       "          4582.5601, 4335.3799])),\n",
       " (tensor(0), tensor(0), tensor(1), tensor(0), tensor(0)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stream = eegdatastreamer[\"train\"].stream()\n",
    "next(iter(train_stream))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
